### 先说结论
这是我正儿八经参加的第一个数据科学竞赛，心路历程也是十分艰辛，队友经历几次更换，自己也是经常游走在崩溃的边缘，同门都说我头发又见秃，总之不是很顺利的，最后结果不是很理想。用我队友的话就是100来人的比赛，不拿个top3真的没用。
**菜是原罪**。😢
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222110352308.png)这个比赛总结，拖了太久了，自己真的是拖延癌晚期，写出来对别人实际参考意义不大，只是对我自己的一个总结，想在这个基础上把代码整理一下，争取下次别这么丢人了～
### [百度点石](https://dianshi.baidu.com/index)
先来谈谈百度点石吧，常见的比赛阵地是天池、kaggle、datacastle等等，点石算是比较小众的吧，含金量不高，参赛人数较少，倒是之前因为他和我交合作了一个比赛有点印象。

再也不想参加点石的比赛了，有问题官方不管不顾，题目不规范，对新手最终要的论坛也存在活跃度几乎为0的情况，体验感超差，太不友好了，费心费神。如果你和我一样还是入门新手的话，建议还是平台大一些的天池或者kaggle吧，和一些热爱分享的人一起比赛，进步会很快的～

**一次管够，再也不想参加了。**

### [赛题背景](https://dianshi.baidu.com/competition/24/question)
学计算机还是需要点英文阅读能力的～
如果你读着很累，还是选择有道滑词翻译插件吧～
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190222111649815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FsaWNlbG14,size_16,color_FFFFFF,t_70)

### 数据探索
#### 获取数据
数据呈现的特点是：真实脱敏数据，**数据庞大，缺失值很多**，原始特征共21维。
当训练数据下载下来的时候，我是懵逼的：
1. 原始数据以二进制文件存储，以前没碰到过。
2. 好不容易将数据切分开以后，发现训练数据24G，测试数据8G。
咨询了大佬以后，我认为解决办法有三个：选择spark or 借一台至少64G内存的服务器 或者选择将数据拆分开一组一组的跑模型～
最终，厚着脸皮找同学借到了他们实验室的服务器，尽管有64G内存，但是还是时刻在崩掉的边缘啊～

#### 数据
```python
    用户id
    性别
    年龄
    教育程度
    地域编码
    是否留存
    安装渠道
    视频id
    视频分类
    视频tag
    创作者
    上传时间
    视频时长
    是否展现
    是否点击
    推荐类型
    视频播放时长
    行为发生时间戳
    是否评论
    是否点赞
    是否转发
```
1. 数据特点
因为是真实数据，所以**缺失值非常多**，还存在很多异常数据，比如这个用户一会是男生一会是女人。
2. 滑窗法（pass）
一般这样的问题呢，大多数都是时候**滑窗法**，对一段时间的数据进行分割，划分成多个自数据集（如果你还没了解过滑窗法，请戳[这里](https://blog.csdn.net/oXiaoBuDianEr123/article/details/79309022)），但是虽然此次数据的时间跨度如下
```python
训练集 10.15-10.31
测试集 10.15-10.31
预测：11.1日（周四的留存率）
```
但是每个用户在这段时间内只有一天有数据，所以滑窗法失效。

3. 各个特征与是否留存的相关性
这段需要绘图来查看，这种常用的小代码，我会更新在文末，总结成一个专门的笔记。
绘图之后，都差不多，**没有特别相关和特别无关的特征**，这又给我们增加了难度。
#### 数据整理
在最开始的时候我们纠结的点就在于，一个是滑窗法对此题该怎么应用，另一个就是如何填充这里大量的缺失数据。
事实证明，年轻人拿到数据就该多自己观察数据，而不要就想着我该怎么处理。这两个问题都是无用的， 因为是线上数据，所以必然有很多缺失值，而且也是合理存在的，同时lightgbm是对缺失值不敏感的，所以还是经验太少了，在此处耽误了很多时间。
主要的处理数据步骤：
	1. 删除整份数据都相同的特征。
	2. 规范异常数据。
	3. 对数据按照uid和timestamp_of_behavior升序排序。
	4. 对必要特征处理缺失值和异常值，比如性别的异常。将所以的缺失值替换成nan。
	5. 时间戳转化为日期。
	6. 连续特征离散化，离散属性数字化。
	7. 转化数据类型，降低内存使用率。
	8. 最后将原始数据整理成一个用户一条数据的形式。（我觉得我成绩不好的原因绝对是这里出了问题）
#### 我的问题？？？反思自己一下吧～
1. 数据观察不够，不够详细，也可能是没有经验吧，只知道观察数据，但是忽略了“如何做”。
2. 这块没有一步到位的处理好，写的bug太多，导致到最后的时间内我还在修改这块的代码。
3. 没有及时将数据保存，节省代码运行时间。我在前面写过，这次比赛的数据量非常大，运行一次简简单单的数据处理，可能就需要个把个小时，所以要随机应变啊～
4. 变通性太差，一直在纠结，殊不知只有实践才是检验真理的唯一标准，别犹豫，多查多做即可。
5. 前期积累太差，没有做好常用小函数的笔记，写的时候翻书，翻别人博客浪费了很多时间。《利用Python进行数据分析》一定要多翻多看多用，我就是太久不用，非常生疏。

### 特征工程
特征工程的话，我还从user、item、user-item这三方面进行展开的，但是为啥最终效果这么差，我觉得旁人没法给我一个解释，还需要我自己在去读别人的代码，来进行自我反思吧。
我阅读了几个大牛开源的思路后整理一个这个，仅供新手使用，大牛勿喷～
[特征工程的特征从何而来](https://github.com/alicelmx/BaiduHanKanPredict/blob/master/%E7%AE%97%E6%B3%95%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93.pdf)

#### 反思
在做特征的时候就注定我是个loser了吧，后期进行代码排查的时候我发现我写的几个地方就把数据大变样了，造成了很多问题，虽然后期纠正了，但是还是逃不过命运。在一个地方就是我没有掌握如何判断新加特征是否能提高AUC，我就一味的加特征加特征，我觉得十分忙目，不知道如何处理。还有的就是在昨晚特征之后也是可以把数据保存成一个新数据的，节省时间，最后一次模型跑了8个多小时，都快哭了。
### 训练模型
因为是第一次参加这种比赛，我就选用了对新手最友好的lgb训练了一个模型，这个比xgb训练速度会更快一些，效果也差不多。我没有用网格搜索来挑选最合适的参数，我觉得这点影响都是微乎其微的可以忽略。在这一阶段我最大的收获就是学习了lgb这种数据科学比赛大杀器，**[官方文档](https://lightgbm.readthedocs.io/en/latest/)一生推**，你想要的官方文档都有。
### 模型融合
到训练模型我的历程也就结束了，都没做到模型融合的一步，真的是遗憾。
奉上我之前找到的非常好的博文：
https://blog.csdn.net/WxyangID/article/details/80205075
### 人生经验
1. **练习赛和真实比赛绝对不一样**！！！比飞行棋更加紧张刺激～
2. **团队协作**很重要，如果你是新手菜鸟，一定要有队友，孤军奋战真的太累～
3. **找到适合的入坑比赛**。尽量选择预测、分类这种常见场景的，数据集大小要适中，像我这次就是数据25G的数据，跑一次模型几个小时，要快速迭代才能给我们更多反馈，进行更多优化。
4. 熟能生巧。表格型比赛大同小异，打上一两次就能积累非常多的经验，要有自信，也要把一些常见的代码积累下来，都是可复用的。
5. **版本控制**。无论是你自己还是队友之间都需要进行版本控制，才能不用浪费一遍又一遍的时间来记住我哪里做了改动，而且现在github开放私人库了，我觉得是十分友好的。
6. 海量数据处理时记得**降低内存使用**，优化存储空间，我就因为对pandas数据处理机制不理解，没有好好优化，跑崩过一次又一次。
7. **代码的耦合性**一定要降低啊，一定要降低，我把数据处理、特征工程、和模型都放在一起，后期好痛苦- -。
8. 学会**多读别人的源码**，学习思路，学习代码优化，这样才能快速提升。
良心推荐，一个[top solution集合](https://github.com/Smilexuhc/Data-Competition-TopSolution)，一个成电的学弟进行总结的。
9. **心态要好**，打比赛嘛，肯定是要适当熬夜的，但是别让自己太暴躁。
### 第三名的简短经验分享
https://developer.baidu.com/forum/topic/show/298191
### 结语
写到最后，发现自己并没有写太多的干货，特别重要的特征工程都是一笔带过，因为自己真的菜吧，好多问题还是没理解，真的很惭愧。

